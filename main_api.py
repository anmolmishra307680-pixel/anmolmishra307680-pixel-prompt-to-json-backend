"""FastAPI Backend for Prompt-to-JSON System"""

from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import APIKeyHeader
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
import uvicorn
from datetime import datetime, timezone
import os
import secrets
import logging
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Import agents and database
from prompt_agent import MainAgent
from evaluator import EvaluatorAgent
from rl_agent import RLLoop
from db.database import Database
from feedback import FeedbackAgent
from cache import cache
from auth import create_access_token, get_current_user
from errors import register_exception_handlers

from fastapi.security import HTTPBearer

# Version constant for consistency
API_VERSION = "2.1.1"

# Define fallback classes at module level for better performance
class FallbackAgent:
    def run(self, *args, **kwargs):
        return {"error": "Agent not available"}

class FallbackDB:
    def get_session(self): 
        raise RuntimeError("Database unavailable")
    def save_spec(self, *args): return "fallback_id"
    def save_eval(self, *args): return "fallback_id"
    def get_report(self, *args): return None
    def get_iteration_logs(self, *args): return []
    def save_hidg_log(self, *args): return "fallback_id"

API_KEY = os.getenv("API_KEY")
if not API_KEY:
    raise ValueError("API_KEY environment variable is required")
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)
bearer_scheme = HTTPBearer(auto_error=False)

def verify_api_key(api_key: str = Depends(api_key_header)):
    """Verify API key from X-API-Key header"""
    if not api_key or not secrets.compare_digest(api_key, API_KEY):
        raise HTTPException(
            status_code=401, 
            detail="Invalid or missing API key. Include X-API-Key header."
        )
    return api_key

app = FastAPI(
    title="Prompt-to-JSON API", 
    version=API_VERSION,
    description="Production-Ready AI Backend with Multi-Agent Coordination",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_tags=[
        {"name": "Authentication", "description": "JWT token and API key authentication"},
        {"name": "AI Agents", "description": "AI specification generation and evaluation"},
        {"name": "Monitoring", "description": "Health checks and system metrics"}
    ]
)

# Register structured exception handlers
register_exception_handlers(app)

# Custom OpenAPI schema with security
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    
    from fastapi.openapi.utils import get_openapi
    openapi_schema = get_openapi(
        title="Prompt-to-JSON API",
        version=API_VERSION,
        description="Production-Ready AI Backend with Multi-Agent Coordination",
        routes=app.routes,
    )
    
    # Add security schemes
    openapi_schema["components"]["securitySchemes"] = {
        "APIKeyHeader": {
            "type": "apiKey",
            "in": "header",
            "name": "X-API-Key"
        },
        "BearerAuth": {
            "type": "http",
            "scheme": "bearer",
            "bearerFormat": "JWT"
        }
    }
    
    # Apply security to all endpoints
    for path, path_item in openapi_schema["paths"].items():
        for operation in path_item.values():
            if isinstance(operation, dict) and "operationId" in operation:
                if path == "/token":
                    # Token endpoint is public (no auth required)
                    operation["security"] = []
                elif path == "/metrics":
                    # Metrics endpoint is public for monitoring
                    operation["security"] = []
                else:
                    # All other endpoints need both API key and JWT
                    operation["security"] = [
                        {"APIKeyHeader": [], "BearerAuth": []}
                    ]
    
    app.openapi_schema = openapi_schema
    return app.openapi_schema

app.openapi = custom_openapi

# Rate limiter with slowapi
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS middleware - only allow authorized frontends
FRONTEND_URL = os.getenv("FRONTEND_URL", "*")
allowed_origins = [FRONTEND_URL] if FRONTEND_URL != "*" else ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# Sentry monitoring with performance tracing
try:
    import sentry_sdk
    from sentry_sdk.integrations.asgi import SentryAsgiMiddleware
    from sentry_sdk.integrations.fastapi import FastApiIntegration
    from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration
    
    sentry_dsn = os.getenv("SENTRY_DSN")
    if sentry_dsn:
        sentry_sdk.init(
            dsn=sentry_dsn,
            environment=os.getenv("SENTRY_ENVIRONMENT", "development"),
            traces_sample_rate=0.1,  # 10% of transactions for performance monitoring
            profiles_sample_rate=0.1,  # 10% for profiling
            integrations=[
                FastApiIntegration(auto_enabling_integrations=True),
                SqlalchemyIntegration(),
            ],
            attach_stacktrace=True,
            send_default_pii=False,  # Don't send personally identifiable information
        )
        app.add_middleware(SentryAsgiMiddleware)
        print(f"[OK] Sentry monitoring enabled for {os.getenv('SENTRY_ENVIRONMENT', 'development')}")
except ImportError:
    print("[WARN] Sentry not available - install: pip install sentry-sdk")

# Prometheus metrics
try:
    from prometheus_fastapi_instrumentator import Instrumentator
    
    # Initialize Prometheus instrumentator
    instrumentator = Instrumentator(
        should_group_status_codes=False,
        should_ignore_untemplated=True,
        should_instrument_requests_inprogress=True,
        inprogress_name="http_requests_inprogress",
        inprogress_labels=True,
    )
    
    # Instrument the app and expose metrics
    instrumentator.instrument(app)
    instrumentator.expose(app, endpoint="/metrics")
    
    print("[OK] Prometheus metrics enabled at /metrics")
except ImportError:
    print("[WARN] Prometheus not available - install: pip install prometheus-fastapi-instrumentator")

# Initialize agents and database with error handling
try:
    prompt_agent = MainAgent()
    evaluator_agent = EvaluatorAgent()
    rl_agent = RLLoop()
    feedback_agent = FeedbackAgent()
    db = Database()
    print("[OK] All agents initialized successfully")
except Exception as e:
    print(f"[WARN] Agent initialization warning: {e}")
    # Create minimal fallback objects
    class FallbackAgent:
        def run(self, *args, **kwargs):
            return {"error": "Agent not available"}
    
    prompt_agent = FallbackAgent()
    evaluator_agent = FallbackAgent()
    rl_agent = FallbackAgent()
    feedback_agent = FallbackAgent()
    try:
        db = Database()
    except Exception as db_error:
        print(f"[ERROR] Database initialization failed: {db_error}")
        db = FallbackDB()

# Request models
class GenerateRequest(BaseModel):
    prompt: str

class EvaluateRequest(BaseModel):
    spec: Dict[Any, Any]
    prompt: str

class IterateRequest(BaseModel):
    prompt: str
    n_iter: int = 3

# Response Models
class StandardResponse(BaseModel):
    success: bool
    message: str = ""
    data: Optional[Dict[Any, Any]] = None

class EvaluationResponse(StandardResponse):
    evaluation_id: Optional[str] = None
    evaluation: Optional[Dict[Any, Any]] = None

class IterationResponse(StandardResponse):
    session_id: Optional[str] = None
    total_iterations: Optional[int] = None
    iterations: Optional[List[Dict[Any, Any]]] = None

class LogValuesRequest(BaseModel):
    date: str
    day: str
    task: str
    values_reflection: Dict[str, str]
    achievements: Dict[Any, Any] = None
    technical_notes: Dict[Any, Any] = None

class TokenRequest(BaseModel):
    username: str
    password: str

@app.post("/token")
@limiter.limit("10/minute")
def token_create(request: Request, payload: TokenRequest):
    """Create JWT token for authentication"""
    username = payload.username
    password = payload.password
    if not username or not password:
        raise HTTPException(status_code=400, detail="username and password required")
    
    # Check against environment variables for security
    demo_username = os.getenv("DEMO_USERNAME")
    demo_password = os.getenv("DEMO_PASSWORD")
    
    if not demo_username or not demo_password:
        raise HTTPException(status_code=500, detail="Authentication not configured")
    
    if username == demo_username and password == demo_password:
        token = create_access_token({"sub": username})
        return {"access_token": token, "token_type": "bearer"}
    
    raise HTTPException(status_code=401, detail="Invalid credentials")

@app.get("/")
@limiter.limit("20/minute")
async def root(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Root endpoint"""
    return {
        "message": "Prompt-to-JSON API", 
        "version": API_VERSION,
        "status": "Production Ready",
        "features": ["AI Agents", "Multi-Agent Coordination", "RL Training", "JWT Authentication", "Monitoring"]
    }



@app.get("/health")
@limiter.limit("20/minute")
async def health_check(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Health check endpoint"""
    try:
        # Test database connection
        session = db.get_session()
        session.close()
        db_status = True
    except Exception as e:
        db_status = False
        print(f"Database health check failed: {e}")
    
    # Test agent availability
    agents_status = []
    for name, agent in [("prompt", prompt_agent), ("evaluator", evaluator_agent), ("rl", rl_agent)]:
        if hasattr(agent, 'run'):
            agents_status.append(name)
    
    return {
        "status": "healthy" if db_status else "degraded",
        "database": db_status,
        "agents": agents_status,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.get("/basic-metrics")
@limiter.limit("20/minute")
async def basic_metrics(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Basic metrics endpoint"""
    try:
        from pathlib import Path
        
        # Count generated files
        specs_count = len(list(Path("spec_outputs").glob("*.json"))) if Path("spec_outputs").exists() else 0
        reports_count = len(list(Path("reports").glob("*.json"))) if Path("reports").exists() else 0
        logs_count = len(list(Path("logs").glob("*.json"))) if Path("logs").exists() else 0
        
        return {
            "generated_specs": specs_count,
            "evaluation_reports": reports_count,
            "log_files": logs_count,
            "active_sessions": 0,  # Placeholder for rate_limit_storage
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        return {
            "error": str(e),
            "generated_specs": 0,
            "evaluation_reports": 0,
            "log_files": 0,
            "active_sessions": 0,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.post("/generate")
@limiter.limit("20/minute")
async def generate_spec(request: Request, generate_request: GenerateRequest, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Generate specification from prompt"""
    try:
        spec = prompt_agent.run(generate_request.prompt)
        # Log HIDG entry for generation completion
        try:
            from hidg import log_generation_completion
            log_generation_completion(generate_request.prompt, True)
        except Exception as log_error:
            print(f"HIDG logging error: {log_error}")
        
        return {
            "spec": spec.model_dump(),
            "success": True,
            "message": "Specification generated successfully"
        }
    except Exception as e:
        # Log failed generation
        try:
            from hidg import log_generation_completion
            log_generation_completion(generate_request.prompt, False)
        except Exception as log_error:
            print(f"HIDG logging error: {log_error}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/evaluate")
@limiter.limit("20/minute")
async def evaluate_spec(request: Request, eval_request: EvaluateRequest, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Evaluate specification"""
    try:
        # Import with error handling
        try:
            from schema import DesignSpec
        except ImportError:
            # Fallback if schema not available
            class DesignSpec:
                def __init__(self, **kwargs):
                    for k, v in kwargs.items():
                        setattr(self, k, v)
        
        # Normalize materials to proper format
        spec_data = eval_request.spec.copy()
        
        # Fix materials format - ensure it's a list of MaterialSpec objects
        if "materials" in spec_data:
            materials = spec_data["materials"]
            normalized_materials = []
            for material in materials:
                if isinstance(material, dict):
                    # Already a dict, ensure it has required fields
                    normalized_materials.append({
                        "type": material.get("type", "concrete"),
                        "grade": material.get("grade", None),
                        "properties": material.get("properties", {})
                    })
                elif isinstance(material, str):
                    # Convert string to MaterialSpec format
                    normalized_materials.append({
                        "type": material,
                        "grade": None,
                        "properties": {}
                    })
                else:
                    # Fallback for other types
                    normalized_materials.append({
                        "type": "concrete",
                        "grade": None,
                        "properties": {}
                    })
            spec_data["materials"] = normalized_materials
        
        # Add default values for missing required fields
        if "building_type" not in spec_data:
            spec_data["building_type"] = "general"
        if "stories" not in spec_data:
            spec_data["stories"] = 1
        if "materials" not in spec_data:
            spec_data["materials"] = [{"type": "concrete", "grade": None, "properties": {}}]
        if "dimensions" not in spec_data:
            spec_data["dimensions"] = {"length": 1, "width": 1, "height": 1, "area": 1}
        if "features" not in spec_data:
            spec_data["features"] = []
        if "requirements" not in spec_data:
            spec_data["requirements"] = [eval_request.prompt]
        
        spec = DesignSpec(**spec_data)
        evaluation = evaluator_agent.run(spec, eval_request.prompt)
        
        # Save evaluation and get report ID
        try:
            spec_id = db.save_spec(eval_request.prompt, spec_data, 'EvaluatorAgent')
            report_id = db.save_eval(spec_id, eval_request.prompt, evaluation.model_dump(), evaluation.score)
        except Exception as e:
            print(f"DB save failed: {e}")
            import uuid
            report_id = str(uuid.uuid4())
        
        # Log HIDG entry for evaluation completion
        try:
            from hidg import log_evaluation_completion
            log_evaluation_completion(eval_request.prompt, evaluation.score)
        except Exception as log_error:
            print(f"HIDG logging error: {log_error}")
        
        return {
            "report_id": report_id,
            "evaluation": evaluation.model_dump(),
            "success": True,
            "message": "Evaluation completed successfully"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/iterate")
@limiter.limit("20/minute")
async def iterate_rl(request: Request, iterate_request: IterateRequest, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Run RL iterations with detailed before→after, scores, feedback"""
    try:
        # Ensure minimum 2 iterations
        n_iter = max(2, iterate_request.n_iter)
        rl_agent.max_iterations = n_iter
        
        results = rl_agent.run(iterate_request.prompt, n_iter)
        
        # Format detailed iteration logs
        # Use list comprehension for better performance
        detailed_iterations = [{
            "iteration_number": iteration["iteration"],
            "iteration_id": iteration.get("iteration_id"),
            "before": {
                "spec": iteration.get("spec_before"),
                "score": iteration.get("score_before", 0)
            },
            "after": {
                "spec": iteration["spec_after"],
                "score": iteration["score_after"]
            },
            "evaluation": iteration["evaluation"],
            "feedback": iteration["feedback"],
            "reward": iteration["reward"],
            "improvement": iteration.get("improvement", 0)
        } for iteration in results.get("iterations", [])]
        
        # Clean datetime objects recursively
        def clean_data(data):
            if isinstance(data, dict):
                return {k: clean_data(v) for k, v in data.items()}
            elif isinstance(data, list):
                return [clean_data(item) for item in data]
            elif isinstance(data, datetime):
                return data.isoformat()
            else:
                return data
        
        response_data = {
            "success": True,
            "session_id": results.get("session_id"),
            "prompt": iterate_request.prompt,
            "total_iterations": len(detailed_iterations),
            "iterations": clean_data(detailed_iterations),
            "final_spec": clean_data(results.get("final_spec", {})),
            "learning_insights": clean_data(results.get("learning_insights", {})),
            "message": f"RL training completed with {len(detailed_iterations)} iterations"
        }
        
        # Log HIDG entry for RL training completion
        try:
            from hidg import log_pipeline_completion
            final_score = results.get("learning_insights", {}).get("final_score")
            log_pipeline_completion(iterate_request.prompt, len(detailed_iterations), final_score)
        except Exception as log_error:
            print(f"HIDG logging error: {log_error}")
        
        return response_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/reports/{report_id}")
@limiter.limit("20/minute")
async def get_report(request: Request, report_id: str, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Retrieve full report from DB"""
    try:
        report = db.get_report(report_id)
        if not report:
            raise HTTPException(status_code=404, detail="Report not found")
        return {
            "success": True,
            "report": report
        }
    except Exception as e:
        import logging
        logging.error(f"Failed to retrieve report {report_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve report")

@app.post("/log-values")
@limiter.limit("20/minute")
async def log_values(request: Request, log_request: LogValuesRequest, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Store HIDG values per day"""
    try:
        # Save to database
        hidg_id = db.save_hidg_log(
            log_request.date,
            log_request.day,
            log_request.task,
            log_request.values_reflection,
            log_request.achievements,
            log_request.technical_notes
        )
        
        # Also save to file as backup
        from pathlib import Path
        from datetime import datetime
        import json
        
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        values_entry = {
            "date": log_request.date,
            "day": log_request.day,
            "task": log_request.task,
            "values_reflection": log_request.values_reflection,
            "achievements": log_request.achievements,
            "technical_notes": log_request.technical_notes,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "hidg_id": hidg_id
        }
        
        values_file = logs_dir / "values_log.json"
        values_logs = []
        
        if values_file.exists():
            try:
                with open(values_file, 'r') as f:
                    values_logs = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"Failed to read values log: {e}")
                values_logs = []
        
        values_logs.append(values_entry)
        
        try:
            with open(values_file, 'w') as f:
                json.dump(values_logs, f, indent=2)
        except IOError as e:
            print(f"Failed to write values log: {e}")
            raise HTTPException(status_code=500, detail="Failed to save values log")
        
        import logging
        logging.info(f"Values logged to DB and file: {values_file}")
        
        return {
            "success": True,
            "hidg_id": hidg_id,
            "message": "Values logged successfully",
            "file": str(values_file)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch-evaluate")
@limiter.limit("20/minute")
async def batch_evaluate(request: Request, prompts: List[str], api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Process multiple specs/prompts and store evaluations"""
    try:
        results = []
        for prompt in prompts:
            # Generate spec
            spec = prompt_agent.run(prompt)
            # Evaluate spec
            evaluation = evaluator_agent.run(spec, prompt)
            
            results.append({
                "prompt": prompt,
                "spec": spec.model_dump(),
                "evaluation": evaluation.model_dump()
            })
        
        return {
            "success": True,
            "results": results,
            "count": len(results),
            "message": f"Batch processed {len(results)} prompts"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/iterations/{session_id}")
@limiter.limit("20/minute")
async def get_iteration_logs(request: Request, session_id: str, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Get all iteration logs for a session"""
    try:
        # Try database first
        logs = db.get_iteration_logs(session_id)
        
        # If no logs in DB, check fallback files
        if not logs:
            from pathlib import Path
            import json
            
            iteration_file = Path("logs/iteration_logs.json")
            if iteration_file.exists():
                with open(iteration_file, 'r') as f:
                    all_logs = json.load(f)
                
                # Filter by session_id using list comprehension
                logs = [log for log in all_logs if log.get('session_id') == session_id]
        
        if not logs:
            raise HTTPException(status_code=404, detail="No iteration logs found for this session")
        
        return {
            "success": True,
            "session_id": session_id,
            "total_iterations": len(logs),
            "iterations": logs
        }
    except Exception as e:
        import logging
        logging.error(f"Failed to retrieve iteration logs for session {session_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve iteration logs")

@app.get("/cli-tools")
@limiter.limit("20/minute")
async def get_cli_tools(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Get available CLI tools and commands"""
    # Check database status
    try:
        db.get_session()
        db_status = "✅ Connected (Supabase PostgreSQL)"
        db_tables = "specs, evals, feedback_logs, hidg_logs, iteration_logs"
    except Exception as e:
        db_status = f"❌ Error: {str(e)}"
        db_tables = "Using file fallback (JSON files)"
    
    return {
        "database_status": db_status,
        "database_tables": db_tables,
        "available_endpoints": {
            "/generate": "Generate specifications (requires API key)",
            "/evaluate": "Evaluate specifications (requires API key)", 
            "/iterate": "RL training iterations (requires API key)",
            "/reports/{id}": "Get evaluation reports",
            "/health": "System health check",
            "/metrics": "System metrics"
        },
        "actual_commands": [
            "python main_api.py",
            "python load_test.py",
            "python create-tables.py"
        ],
        "api_key_required": "X-API-Key: <your-api-key> (set via API_KEY environment variable)"
    }

@app.get("/system-test")
@limiter.limit("20/minute")
async def run_system_test(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Run basic system tests"""
    try:
        # Test core functionality
        spec = prompt_agent.run("Test building")
        evaluation = evaluator_agent.run(spec, "Test building")
        
        return {
            "success": True,
            "tests_passed": [
                "prompt_agent",
                "evaluator_agent",
                "database_connection"
            ],
            "message": "All core tests passed"
        }
    except Exception as e:
        import logging
        logging.error(f"System test failed: {e}")
        raise HTTPException(status_code=500, detail=f"System test failed: {str(e)}")

@app.post("/advanced-rl")
@limiter.limit("20/minute")
async def advanced_rl_training(request: Request, rl_request: IterateRequest, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Run Advanced RL training with policy gradients"""
    try:
        from rl_agent.advanced_rl import AdvancedRLEnvironment
        env = AdvancedRLEnvironment()
        
        result = env.train_episode(rl_request.prompt, max_steps=rl_request.n_iter)
        
        return {
            "success": True,
            "prompt": rl_request.prompt,
            "steps": result.get("steps", 0),
            "final_score": result.get("final_score", 0),
            "total_reward": result.get("total_reward", 0),
            "training_file": result.get("training_file", ""),
            "message": "Advanced RL training completed"
        }
    except Exception as e:
        import logging
        logging.error(f"Advanced RL training failed: {e}")
        raise HTTPException(status_code=500, detail=f"Advanced RL training failed: {str(e)}")

@app.post("/admin/prune-logs")
@limiter.limit("20/minute")
async def prune_logs(request: Request, retention_days: int = 30, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Prune old logs for production scalability"""
    try:
        from db.log_pruning import LogPruner
        pruner = LogPruner(retention_days=retention_days)
        results = pruner.prune_all_logs()
        
        return {
            "success": True,
            "retention_days": retention_days,
            "results": results,
            "message": f"Log pruning completed - {results['total_pruned']} entries removed"
        }
    except Exception as e:
        import logging
        logging.error(f"Log pruning failed: {e}")
        raise HTTPException(status_code=500, detail=f"Log pruning failed: {str(e)}")

@app.post("/coordinated-improvement")
@limiter.limit("20/minute")
async def coordinated_improvement(request: Request, request_data: GenerateRequest, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Advanced agent coordination for optimal results"""
    try:
        from agent_coordinator import AgentCoordinator
        coordinator = AgentCoordinator()
        
        result = await coordinator.coordinated_improvement(request_data.prompt)
        
        # Log HIDG entry for coordinated improvement completion
        try:
            from hidg import append_hidg_entry
            final_score = result.get("final_score")
            score_text = f"score:{final_score:.2f}" if final_score else "completed"
            note = f"Multi-agent coordination for '{request_data.prompt[:30]}...' {score_text}"
            append_hidg_entry("COORDINATION", note)
        except Exception as log_error:
            print(f"HIDG logging error: {log_error}")
        
        return {
            "success": True,
            "result": result,
            "message": "Coordinated improvement completed"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/agent-status")
@limiter.limit("20/minute")
async def get_agent_status(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Get status of all AI agents"""
    try:
        from agent_coordinator import AgentCoordinator
        coordinator = AgentCoordinator()
        
        status = coordinator.get_agent_status()
        metrics = coordinator.get_coordination_metrics()
        
        return {
            "success": True,
            "agents": status,
            "coordination_metrics": metrics,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        import logging
        logging.error(f"Failed to get agent status: {e}")
        raise HTTPException(status_code=500, detail="Failed to get agent status")

@app.get("/cache-stats")
@limiter.limit("20/minute")
async def get_cache_stats(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Get cache performance statistics"""
    try:
        stats = cache.get_stats()
        return {
            "success": True,
            "cache_stats": stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        import logging
        logging.error(f"Failed to get cache stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to get cache stats")

@app.get("/system-overview")
@limiter.limit("20/minute")
async def get_system_overview(request: Request, api_key: str = Depends(verify_api_key), user=Depends(get_current_user)):
    """Comprehensive system status and capabilities"""
    try:
        # Get all system information
        health_info = await health_check(request, api_key, user)
        agent_info = await get_agent_status(request, api_key, user)
        cache_info = await get_cache_stats(request, api_key, user)
        metrics_info = await basic_metrics(request, api_key, user)
        
        return {
            "success": True,
            "system_info": {
                "api_version": API_VERSION,
                "production_ready": True,
                "deployment_url": "https://prompt-to-json-backend.onrender.com",
                "features": [
                    "Multi-Agent AI System",
                    "Reinforcement Learning",
                    "Real-time Coordination",
                    "Enterprise Authentication",
                    "Production Monitoring",
                    "High-Performance Caching",
                    "Comprehensive Testing"
                ]
            },
            "health": health_info,
            "agents": agent_info.get("agents", {}),
            "cache": cache_info.get("cache_stats", {}),
            "metrics": metrics_info,
            "endpoints": {
                "total_endpoints": len([route for route in app.routes if hasattr(route, 'methods')]),
                "protected_endpoints": len([route for route in app.routes if hasattr(route, 'methods') and route.path not in ["/token", "/metrics"]]),
                "public_endpoints": len([route for route in app.routes if hasattr(route, 'methods') and route.path in ["/token", "/metrics"]]),
                "authentication_methods": ["API Key", "JWT Token"]
            },
            "performance": {
                "target_response_time": "<200ms",
                "max_concurrent_users": "1000+",
                "uptime_target": "99.9%",
                "rate_limit": "20 requests/minute"
            },
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        import logging
        logging.error(f"Failed to get system overview: {e}")
        raise HTTPException(status_code=500, detail="Failed to get system overview")



if __name__ == "__main__":
    import os
    port = int(os.getenv("PORT", 8000))
    workers = int(os.getenv("MAX_WORKERS", 4))
    
    if os.getenv("PRODUCTION_MODE") == "true":
        # Production configuration - validated for high concurrency
        uvicorn.run(
            "main_api:app",
            host="0.0.0.0",
            port=port,
            workers=workers,
            worker_connections=1000,
            backlog=2048,
            timeout_keep_alive=30
        )
    else:
        # Development configuration
        uvicorn.run(app, host="0.0.0.0", port=port)